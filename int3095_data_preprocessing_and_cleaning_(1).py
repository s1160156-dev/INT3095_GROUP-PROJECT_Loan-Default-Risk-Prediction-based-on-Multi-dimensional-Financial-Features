# -*- coding: utf-8 -*-
"""INT3095_Data_Preprocessing_and_Cleaning (1).ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Vtlkj8TeR0YjZoAdQw3ect97YXJCiY5r
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

from google.colab import drive
drive.mount('/content/drive')

path = "/content/loan_data(1).csv"
df = pd.read_csv(path)

print("Raw data shape:", df.shape)   #Basic data structure and target column inspection

print("\nColumn dtypes:")
print(df.dtypes)

print("\nFirst 5 rows:")
display(df.head())

print("\nTarget variable distribution (loan_status):")
if "loan_status" in df.columns:
    print(df["loan_status"].value_counts(dropna=False))
    print("\nProportion:")
    print(df["loan_status"].value_counts(normalize=True, dropna=False))

df.columns = (
    df.columns.str.strip()
    .str.lower()
    .str.replace(r"[^a-z0-9]+", "_", regex=True)
    .str.strip("_")
)    #Column naming standardization and deduplication

df.drop_duplicates(inplace=True)#Delete completely duplicate rows

null_ratio = df.isna().mean()#Calculate the proportion of missing values ​​in each column.
drop_cols = null_ratio[null_ratio > 0.40].index.tolist()
df.drop(columns=drop_cols, inplace=True)

null_ratio = df.isna().mean().sort_values(ascending=False)
print("Missing value ratio by column:")
display(null_ratio)

non_zero_null = null_ratio[null_ratio > 0]
if not non_zero_null.empty:
    non_zero_null.plot(kind="bar", figsize=(10,4))
    plt.ylabel("Missing ratio")
    plt.title("Missing value ratio by feature")
    plt.tight_layout()
    plt.show()
else:
    print("No missing values in any column.")

drop_cols = null_ratio[null_ratio > 0.40].index.tolist()
print("\nColumns with > 40% missing values to drop:")
print(drop_cols)

df.drop(columns=drop_cols, inplace=True)
print("\nShape after dropping high-missing columns:", df.shape)

target_col = "loan_status"#Standardization of target variables
if target_col in df.columns:

    df[target_col] = (
        df[target_col]
        .astype(str)
        .str.strip()
        .str.lower()
        .map({"1":1, "0":0, "yes":1, "no":0, "true":1, "false":0}) # Map the binary categorical variable "Yes/No" to the numerical values ​​1/0
        .fillna(df[target_col].apply(pd.to_numeric, errors="coerce"))
    )

df[target_col] = df[target_col].fillna(0).astype(int)

num_cols_auto = df.select_dtypes(include=[np.number]).columns.tolist()#Automatically collect all numerical feature column names.
cat_cols_auto = df.select_dtypes(exclude=[np.number]).columns.tolist()#Automatically collect all non-numerical feature column names.

likely_cats = [
    "person_gender","person_education","person_home_ownership",
    "loan_intent","previous_loan_defaults_on_file"
]
for c in likely_cats:
    if c in df.columns:
        df[c] = df[c].astype(str).str.strip().str.lower()

        if c == "previous_loan_defaults_on_file":
            df[c] = df[c].map({"yes":1, "no":0}).fillna(df[c])

            if not np.issubdtype(df[c].dtype, np.number):
                df[c] = pd.to_numeric(df[c], errors="coerce")#Categorical and numerical feature normalization

likely_nums = [
    "person_age","person_income","person_emp_exp","loan_amnt","loan_int_rate",
    "loan_percent_income","cb_person_cred_hist_length","credit_score"
]
for c in likely_nums:
    if c in df.columns:
        df[c] = pd.to_numeric(df[c], errors="coerce")

num_cols_overview = [
    "person_age", "person_income", "loan_amnt",
    "loan_int_rate", "loan_percent_income", "credit_score"
]#Numerical distribution overview and visualization

for col in num_cols_overview:
    if col in df.columns:
        print(f"\n=== {col} statistics ===")
        display(df[col].describe())

        df[col].hist(bins=30, figsize=(4,3))
        plt.title(f"Distribution of {col}")
        plt.xlabel(col)
        plt.ylabel("Count")
        plt.tight_layout()
        plt.show()

bounds = {
    "person_age": (18, 100),
    "person_emp_exp": (0, 60),
    "person_income": (0, None),
    "loan_amnt": (0, None),
    "loan_int_rate": (0, 40),
    "loan_percent_income": (0, 1.5),
    "cb_person_cred_hist_length": (0, 50),
    "credit_score": (300, 850)
}#Reasonable value range constraints
for c, (lo, hi) in bounds.items():
    if c in df.columns:
        if lo is not None:
            df.loc[df[c] < lo, c] = np.nan
        if hi is not None:
            df.loc[df[c] > hi, c] = np.nan

"""Missing value imputation"""

num_cols = df.select_dtypes(include=[np.number]).columns.tolist()
cat_cols = df.select_dtypes(exclude=[np.number]).columns.tolist()

for c in num_cols:
    if c != target_col:
        med = df[c].median()
        df[c] = df[c].fillna(med)

for c in cat_cols:
    df[c] = df[c].fillna("unknown")

"""One-Hot encoding and deletion of columns with no information"""

cols_to_encode = [c for c in cat_cols if c != target_col]
df = pd.get_dummies(df, columns=cols_to_encode, drop_first=True)

nunique = df.nunique()
const_cols = nunique[nunique <= 1].index.tolist()
if const_cols:
    df.drop(columns=const_cols, inplace=True)

"""Highly relevant feature screening"""

corr = df.corr(numeric_only=True).abs()
upper = corr.where(np.triu(np.ones(corr.shape), k=1).astype(bool))
to_drop_corr = [column for column in upper.columns if any(upper[column] > 0.98)]
if to_drop_corr:
    #
    to_drop_corr = [c for c in to_drop_corr if c != target_col]
    df.drop(columns=to_drop_corr, inplace=True, errors="ignore")

"""DTI and Interest Rate * Credit Score"""

if {"loan_amnt","person_income"}.issubset(df.columns):
    df["feat_dti_approx"] = (df["loan_amnt"] / (df["person_income"] + 1e-9)).clip(0, 2)

if {"loan_int_rate","credit_score"}.issubset(df.columns):
    df["feat_rate_x_score"] = df["loan_int_rate"] * df["credit_score"]

"""Column order adjustment and result saving"""

cols = df.columns.tolist()
if target_col in cols:
    cols = [target_col] + [c for c in cols if c != target_col]
    df = df[cols]

clean_path = "/content/loan_data_clean.csv"
df.to_csv(clean_path, index=False)

print("Clean data shape:", df.shape)
print("Saved to:", clean_path)
print(df.head(10))