# -*- coding: utf-8 -*-
"""INT3095 Data Analysis and Visualization.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Vm0yr3cjB6h47tOitPMnHEEzY9pJihgt
"""

from google.colab import drive
import pandas as pd
import os
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import warnings
warnings.filterwarnings('ignore')

drive.mount('/content/drive')

# Commented out IPython magic to ensure Python compatibility.
# %run /content/drive/MyDrive/INT3095Loan_Default_Project/INT3095_Model_Training_and_Evaluation.ipynb

print(" Model Type:",type(model_rf))
print(" Test Set Shape:",X_test_drop.shape)
print(" Best Threshold:",best_thr_rf)
print(" Risk Score Sample:",model_rf.predict_proba(X_test_drop)[:3, 1])
print("\nAll variables inherited successfully")

sns.set_style("whitegrid")

print("1.Financial Feature Importance Analysis")

imp = pd.Series(model_rf.feature_importances_, index=X_test_drop.columns).sort_values(ascending=False)

dim_map = {
    "Income": ["person_income", "loan_percent_income", "feat_dti_approx"],
    "Debt": ["loan_amnt", "loan_int_rate", "feat_dti_approx"],
    "Credit": ["credit_score", "cb_person_cred_hist_length", "person_age"],
    "History": ["previous_loan_defaults_on_file", "person_emp_exp"],
}

dim_imp = pd.Series({k: imp[imp.index.intersection(v)].sum() for k, v in dim_map.items()}).sort_values(ascending=False)

fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))
dim_imp.plot(kind="barh", ax=ax1, color="#4ecdc4"); ax1.set_title("Feature Importance by Dimension")
imp.head(10).iloc[::-1].plot(kind="barh", ax=ax2, color="#ff6b6b"); ax2.set_title("Top 10 Features")
plt.tight_layout(); plt.savefig("d1_importance.png", dpi=300); plt.show()

n = 4; fig, axes = plt.subplots(2, 2, figsize=(12, 10))
for i, f in enumerate(["person_income", "credit_score", "loan_int_rate", "loan_amnt"]):
    if f in X_test_drop.columns:
        tmp = pd.DataFrame({f: X_test_drop[f], "y": y_test.values}).dropna()
        tmp["bin"] = pd.qcut(tmp[f], q=5, duplicates="drop")
        g = tmp.groupby("bin")["y"].mean()
        axes[i//2, i%2].plot(range(len(g)), g.values, marker="o", linewidth=2)
        axes[i//2, i%2].set_title(f"{f} vs Default Rate"); axes[i//2, i%2].set_ylim(0, 1)
plt.tight_layout(); plt.savefig("d1_default_bins.png", dpi=300); plt.show()

corr_feats = [c for c in ["person_income", "credit_score", "loan_int_rate", "person_age",
                           "loan_amnt", "loan_percent_income", "feat_dti_approx",
                           "cb_person_cred_hist_length"] if c in X_test_drop.columns]
plt.figure(figsize=(10, 8))
sns.heatmap(X_test_drop[corr_feats].corr(), annot=True, fmt=".2f", cmap="coolwarm", cbar_kws={'label': 'Correlation'})
plt.title("Financial Features Correlation Matrix"); plt.tight_layout(); plt.savefig("d1_corr_heatmap.png", dpi=300); plt.show()

print("2.Risk Stratification Analysis")

y_true=y_test.values
risk_score=model_rf.predict_proba(X_test_drop)[:,1]

risk_df = X_test_drop.copy()
risk_df["y"] = y_true
risk_df["score"] = risk_score

risk_df["level"] = pd.qcut(risk_df["score"],q=5,duplicates="drop")
risk_df["level_name"] =pd.cut(risk_df["level"].cat.codes,
                                bins=[-1,0,1,2,3,4],
                                labels=["Very Low","Low", "Medium","High","Very High"])

agg = risk_df.groupby("level_name", observed=True).agg(
    default_rate=("y","mean"),
    count=("y","size"),
    income=("person_income","mean"),
    credit=("credit_score","mean"),
    rate=("loan_int_rate","mean"),
    amount=("loan_amnt","mean"))

print(agg.round(3));print()

fig, axes = plt.subplots(3,2,figsize=(14,12))
axes = axes.flatten()
for i, (m,t) in enumerate(zip(
    ["default_rate", "count", "income", "credit", "rate", "amount"],
    ["Default Rate", "Sample Count", "Avg Income", "Avg Credit Score",
     "Avg Interest Rate", "Avg Loan Amount"])):
    axes[i].bar(range(len(agg)),agg[m].values,
                color=["#2ecc71","#f1c40f","#e67e22","#e74c3c","#c0392b"])
    axes[i].set_xticks(range(len(agg)))
    axes[i].set_xticklabels(agg.index,rotation=30)
    axes[i].set_title(t)

plt.suptitle("Risk Stratification Analysis", fontsize=14, fontweight='bold')
plt.tight_layout()
plt.savefig("d2_risk_layers.png", dpi=300)
plt.show()

print("3.Risk Pricing Strategy")

base_rate = 5.0
risk_premium=[0.5,1.5,3.5,6.0,9.5]
final_rate=[base_rate + p for p in risk_premium]
max_amount=[35000,30000,20000,12000,5000]
n_risks = len(agg.index)

pricing = pd.DataFrame({
    "Risk_Level": agg.index.tolist(),
    "Default_Rate_%": np.round(agg["default_rate"].values * 100,1),
    "Base_Rate_%": [base_rate] * n_risks,
    "Risk_Premium_%": risk_premium[:n_risks],
    "Final_Rate_%": final_rate[:n_risks],
    "Max_Loan_Amount": max_amount[:n_risks],})
pricing.to_csv("d3_pricing_framework.csv", index=False)
print(pricing)
print()

colors=["#2ecc71","#f1c40f","#e67e22","#e74c3c","#c0392b"][:n_risks]
fig,axes = plt.subplots(1,3,figsize=(16,5))

axes[0].bar(range(n_risks),final_rate[:n_risks],color=colors)
axes[0].set_xticks(range(n_risks))
axes[0].set_xticklabels(agg.index, rotation=30)
axes[0].set_title("Recommended Interest Rate")
axes[0].set_ylabel("Interest Rate %")

axes[1].bar(range(n_risks), max_amount[:n_risks], color=colors)
axes[1].set_xticks(range(n_risks))
axes[1].set_xticklabels(agg.index, rotation=30)
axes[1].set_title("Recommended Max Loan Amount")
axes[1].set_ylabel("Amount ($)")

axes[2].plot(range(n_risks), agg["default_rate"].values * 100, marker="o", linewidth=2.5)
ax2 = axes[2].twinx()
ax2.plot(range(n_risks), final_rate[:n_risks], marker="s", linewidth=2.5, color="red")
axes[2].set_xticks(range(n_risks))
axes[2].set_xticklabels(agg.index, rotation=30)
axes[2].set_title("Risk-Return Balance")

plt.suptitle("Risk Pricing and Strategy",fontsize=14, fontweight='bold')
plt.tight_layout()
plt.savefig("d3_pricing_plots.png",dpi=300)
plt.show()

print("4.Model Blind Spots and Improvement Suggestions")

y_pred=(risk_score >= best_thr_rf).astype(int)
fp=(y_pred==1)&(y_test==0)
fn=(y_pred==0)&(y_test==1)
tn=(y_pred==0)&(y_test==0)
tp=(y_pred==1)&(y_test==1)

print(f"Confusion Matrix:TN={tn.sum()},TP={tp.sum()},FP={fp.sum()},FN={fn.sum()}\n")

fig, axes = plt.subplots(2,2,figsize=(14,10))
for i,(f,title) in enumerate([("person_income","Income"),("credit_score","Credit Score")]):
    if f in X_test_drop.columns:
        data =[X_test_drop.loc[tn,f],X_test_drop.loc[fp, f],
                X_test_drop.loc[fn,f], X_test_drop.loc[tp, f]]
        bp = axes[i,0].boxplot(data,labels=["TN","FP","FN","TP"], patch_artist=True)
        for patch, color in zip(bp['boxes'],['#2ecc71','#f39c12','#e74c3c','#c0392b']):
            patch.set_facecolor(color)
        axes[i,0].set_title(f"{f} Distribution")
        axes[i,0].grid(axis='y',alpha=0.3)

axes[0,1].hist(risk_score[fp],bins=30,alpha=0.6,label=f"FP ({fp.sum()})",color='#f39c12')
axes[0,1].hist(risk_score[fn],bins=30,alpha=0.6,label=f"FN ({fn.sum()})",color='#e74c3c')
axes[0,1].axvline(best_thr_rf, color="red", linestyle="--",linewidth=2)
axes[0,1].set_title("Misclassified Samples")
axes[0,1].set_xlabel("Predicted Default Probability")
axes[0,1].legend()

improvement =pd.DataFrame({
    "Issue": ["Too Many FP","Too Many FN","Feature Gaps"],
    "Recommendation": ["Adjust threshold","Weight risk features","Add behavioral data"],
    "Priority": ["High","High","Medium"]})
improvement.to_csv("d4_improvement.csv",index=False)
print(improvement)

axes[1,1].axis('off')
axes[1,1].set_title("Improvement Roadmap", fontweight='bold')

plt.suptitle("Model Blind Spots and Optimization",fontsize=14,fontweight='bold')
plt.tight_layout()
plt.savefig("d4_blind_spots.png",dpi=300)
plt.show()

"""Dimensions 1 to 4 Complete"""